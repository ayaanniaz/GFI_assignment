{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb3d79f2-3357-447a-b455-b74f360ae303",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company Name</th>\n",
       "      <th>Company Description</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Website URL</th>\n",
       "      <th>Linkedin URL</th>\n",
       "      <th>Careers Page URL</th>\n",
       "      <th>Job listings page URL</th>\n",
       "      <th>job post1 URL</th>\n",
       "      <th>job post1 title</th>\n",
       "      <th>job post2 URL</th>\n",
       "      <th>job post2 title</th>\n",
       "      <th>job post3 URL</th>\n",
       "      <th>job post3 title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Forest Stewardship Council</td>\n",
       "      <td>Promoting sustainable forestry to combat clima...</td>\n",
       "      <td>Example1-</td>\n",
       "      <td>https://fsc.org/en</td>\n",
       "      <td>https://www.linkedin.com/company/fsc-internati...</td>\n",
       "      <td>https://fsc.org/en/careers-at-fsc</td>\n",
       "      <td>https://fsc.org/en/careers-at-fsc</td>\n",
       "      <td>https://fsc.jobs.personio.com/job/2262148?lang...</td>\n",
       "      <td>Market Intelligence Manager (m-f-d)</td>\n",
       "      <td>https://fsc.jobs.personio.com/job/2268639?lang...</td>\n",
       "      <td>Trademark Manager (m-f-d)</td>\n",
       "      <td>https://fsc.jobs.personio.com/job/2262183?lang...</td>\n",
       "      <td>Value Chain Development Manager (m-f-d)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Polestar</td>\n",
       "      <td>Driving society forward with uncompromised ele...</td>\n",
       "      <td>Example2-</td>\n",
       "      <td>https://www.polestar.com/</td>\n",
       "      <td>https://www.linkedin.com/company/polestarcars/...</td>\n",
       "      <td>https://www.polestar.com/global/about/careers/</td>\n",
       "      <td>https://polestar.teamtailor.com/jobs</td>\n",
       "      <td>https://polestar.teamtailor.com/jobs/6551682-p...</td>\n",
       "      <td>Planning, Ordering &amp; Distribution Manager - Pa...</td>\n",
       "      <td>https://polestar.teamtailor.com/jobs/6538269-f...</td>\n",
       "      <td>Financial Accounting Manager - UK</td>\n",
       "      <td>https://polestar.teamtailor.com/jobs/6517579-r...</td>\n",
       "      <td>Retail Operation Manager</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sweep</td>\n",
       "      <td>Streamline carbon management and reduction acr...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Climate Bonds Initiative</td>\n",
       "      <td>Mobilizing $100 trillion bond market for globa...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fuse Energy</td>\n",
       "      <td>Accelerating global renewable energy transitio...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>Power Factors</td>\n",
       "      <td>Accelerating the energy transition with softwa...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>Ecology Project International</td>\n",
       "      <td>Youth-driven conservation through local expert...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>Enverus</td>\n",
       "      <td>Accelerating investment in efficient, renewabl...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>FlexGen</td>\n",
       "      <td>Advanced energy storage solutions reducing env...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>The Aspen Institute</td>\n",
       "      <td>Driving global climate solutions through dialo...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>173 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Company Name  \\\n",
       "0       Forest Stewardship Council   \n",
       "1                         Polestar   \n",
       "2                            Sweep   \n",
       "3         Climate Bonds Initiative   \n",
       "4                      Fuse Energy   \n",
       "..                             ...   \n",
       "168                  Power Factors   \n",
       "169  Ecology Project International   \n",
       "170                        Enverus   \n",
       "171                        FlexGen   \n",
       "172            The Aspen Institute   \n",
       "\n",
       "                                   Company Description Unnamed: 2  \\\n",
       "0    Promoting sustainable forestry to combat clima...  Example1-   \n",
       "1    Driving society forward with uncompromised ele...  Example2-   \n",
       "2    Streamline carbon management and reduction acr...        NaN   \n",
       "3    Mobilizing $100 trillion bond market for globa...        NaN   \n",
       "4    Accelerating global renewable energy transitio...        NaN   \n",
       "..                                                 ...        ...   \n",
       "168  Accelerating the energy transition with softwa...        NaN   \n",
       "169  Youth-driven conservation through local expert...        NaN   \n",
       "170  Accelerating investment in efficient, renewabl...        NaN   \n",
       "171  Advanced energy storage solutions reducing env...        NaN   \n",
       "172  Driving global climate solutions through dialo...        NaN   \n",
       "\n",
       "                   Website URL  \\\n",
       "0           https://fsc.org/en   \n",
       "1    https://www.polestar.com/   \n",
       "2                          NaN   \n",
       "3                          NaN   \n",
       "4                          NaN   \n",
       "..                         ...   \n",
       "168                        NaN   \n",
       "169                        NaN   \n",
       "170                        NaN   \n",
       "171                        NaN   \n",
       "172                        NaN   \n",
       "\n",
       "                                          Linkedin URL  \\\n",
       "0    https://www.linkedin.com/company/fsc-internati...   \n",
       "1    https://www.linkedin.com/company/polestarcars/...   \n",
       "2                                                  NaN   \n",
       "3                                                  NaN   \n",
       "4                                                  NaN   \n",
       "..                                                 ...   \n",
       "168                                                NaN   \n",
       "169                                                NaN   \n",
       "170                                                NaN   \n",
       "171                                                NaN   \n",
       "172                                                NaN   \n",
       "\n",
       "                                   Careers Page URL  \\\n",
       "0                 https://fsc.org/en/careers-at-fsc   \n",
       "1    https://www.polestar.com/global/about/careers/   \n",
       "2                                               NaN   \n",
       "3                                               NaN   \n",
       "4                                               NaN   \n",
       "..                                              ...   \n",
       "168                                             NaN   \n",
       "169                                             NaN   \n",
       "170                                             NaN   \n",
       "171                                             NaN   \n",
       "172                                             NaN   \n",
       "\n",
       "                    Job listings page URL  \\\n",
       "0       https://fsc.org/en/careers-at-fsc   \n",
       "1    https://polestar.teamtailor.com/jobs   \n",
       "2                                     NaN   \n",
       "3                                     NaN   \n",
       "4                                     NaN   \n",
       "..                                    ...   \n",
       "168                                   NaN   \n",
       "169                                   NaN   \n",
       "170                                   NaN   \n",
       "171                                   NaN   \n",
       "172                                   NaN   \n",
       "\n",
       "                                         job post1 URL  \\\n",
       "0    https://fsc.jobs.personio.com/job/2262148?lang...   \n",
       "1    https://polestar.teamtailor.com/jobs/6551682-p...   \n",
       "2                                                  NaN   \n",
       "3                                                  NaN   \n",
       "4                                                  NaN   \n",
       "..                                                 ...   \n",
       "168                                                NaN   \n",
       "169                                                NaN   \n",
       "170                                                NaN   \n",
       "171                                                NaN   \n",
       "172                                                NaN   \n",
       "\n",
       "                                       job post1 title  \\\n",
       "0                  Market Intelligence Manager (m-f-d)   \n",
       "1    Planning, Ordering & Distribution Manager - Pa...   \n",
       "2                                                  NaN   \n",
       "3                                                  NaN   \n",
       "4                                                  NaN   \n",
       "..                                                 ...   \n",
       "168                                                NaN   \n",
       "169                                                NaN   \n",
       "170                                                NaN   \n",
       "171                                                NaN   \n",
       "172                                                NaN   \n",
       "\n",
       "                                         job post2 URL  \\\n",
       "0    https://fsc.jobs.personio.com/job/2268639?lang...   \n",
       "1    https://polestar.teamtailor.com/jobs/6538269-f...   \n",
       "2                                                  NaN   \n",
       "3                                                  NaN   \n",
       "4                                                  NaN   \n",
       "..                                                 ...   \n",
       "168                                                NaN   \n",
       "169                                                NaN   \n",
       "170                                                NaN   \n",
       "171                                                NaN   \n",
       "172                                                NaN   \n",
       "\n",
       "                       job post2 title  \\\n",
       "0            Trademark Manager (m-f-d)   \n",
       "1    Financial Accounting Manager - UK   \n",
       "2                                  NaN   \n",
       "3                                  NaN   \n",
       "4                                  NaN   \n",
       "..                                 ...   \n",
       "168                                NaN   \n",
       "169                                NaN   \n",
       "170                                NaN   \n",
       "171                                NaN   \n",
       "172                                NaN   \n",
       "\n",
       "                                         job post3 URL  \\\n",
       "0    https://fsc.jobs.personio.com/job/2262183?lang...   \n",
       "1    https://polestar.teamtailor.com/jobs/6517579-r...   \n",
       "2                                                  NaN   \n",
       "3                                                  NaN   \n",
       "4                                                  NaN   \n",
       "..                                                 ...   \n",
       "168                                                NaN   \n",
       "169                                                NaN   \n",
       "170                                                NaN   \n",
       "171                                                NaN   \n",
       "172                                                NaN   \n",
       "\n",
       "                             job post3 title  \n",
       "0    Value Chain Development Manager (m-f-d)  \n",
       "1                   Retail Operation Manager  \n",
       "2                                        NaN  \n",
       "3                                        NaN  \n",
       "4                                        NaN  \n",
       "..                                       ...  \n",
       "168                                      NaN  \n",
       "169                                      NaN  \n",
       "170                                      NaN  \n",
       "171                                      NaN  \n",
       "172                                      NaN  \n",
       "\n",
       "[173 rows x 13 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"Growth For Impact Data Assignment_Data.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca59689-e9ff-4e38-bfd7-667897ec143d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "e0679eb7-4f1f-4df1-b806-2a3081a27666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Chrome browser...\n",
      "A Chrome window will open - don't close it!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-09 23:14:58,498 WARNING: There was an error managing chromedriver (error decoding response body); using driver found in the cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting website URL finder with Selenium...\n",
      "This uses real Chrome browser - more reliable!\n",
      "\n",
      "Loading previous progress from: GFI_Data_Updated.csv\n",
      "Processing companies 101 to 108 (Total: 173)...\n",
      "\n",
      "[101/173] Searching for: Sightline Climate (CTVC)\n",
      "    ✓ Website already exists: https://www.sightlineclimate.com\n",
      "    → Searching for LinkedIn...\n",
      "    Search query: Sightline Climate (CTVC) datadriven platform linkedin\n",
      "    Found LinkedIn: https://www.linkedin.com/company/ctvc\n",
      "    ✓ LinkedIn saved: https://www.linkedin.com/company/ctvc\n",
      "    ⚠ Could not find website (LinkedIn found)\n",
      "\n",
      "[102/173] Searching for: Pano AI\n",
      "    ✓ Website already exists: https://www.pano.ai\n",
      "    → Searching for LinkedIn...\n",
      "    Search query: Pano AI revolutionizing disaster linkedin\n",
      "    Found LinkedIn: https://www.linkedin.com/company/pano-ai\n",
      "    ✓ LinkedIn saved: https://www.linkedin.com/company/pano-ai\n",
      "    ⚠ Could not find website (LinkedIn found)\n",
      "\n",
      "--- Progress saved: 0 websites found ---\n",
      "\n",
      "[103/173] Searching for: Unirac\n",
      "    ✓ Website already exists: https://unirac.com\n",
      "    → Searching for LinkedIn...\n",
      "    Search query: Unirac driving solar linkedin\n",
      "    Found LinkedIn: https://www.linkedin.com/company/unirac-inc-\n",
      "    ✓ LinkedIn saved: https://www.linkedin.com/company/unirac-inc-\n",
      "    ⚠ Could not find website (LinkedIn found)\n",
      "\n",
      "[104/173] Searching for: Oklo\n",
      "    ✓ Website already exists: https://www.oklo.com\n",
      "    → Searching for LinkedIn...\n",
      "    Search query: Oklo oklos mission linkedin\n",
      "    Found LinkedIn: https://www.linkedin.com/company/oklo\n",
      "    ✓ LinkedIn saved: https://www.linkedin.com/company/oklo\n",
      "    ⚠ Could not find website (LinkedIn found)\n",
      "\n",
      "[105/173] Searching for: Trek Bicycle Corp (Australia)\n",
      "    ✓ Website already exists: https://www.trekbikes.com › en_AU\n",
      "    → Searching for LinkedIn...\n",
      "    Search query: Trek Bicycle Corp (Australia) empowering diverse linkedin\n",
      "    Found LinkedIn: https://www.linkedin.com/company/trekbikes\n",
      "    ✓ LinkedIn saved: https://www.linkedin.com/company/trekbikes\n",
      "    ⚠ Could not find website (LinkedIn found)\n",
      "\n",
      "--- Progress saved: 0 websites found ---\n",
      "\n",
      "[106/173] Searching for: Equilibrium Energy\n",
      "    ✓ Website already exists: https://www.equilibriumenergy.com\n",
      "    → Searching for LinkedIn...\n",
      "    Search query: Equilibrium Energy accelerating clean linkedin\n",
      "    Found LinkedIn: https://www.linkedin.com/company/equilibrium-energy-inc\n",
      "    ✓ LinkedIn saved: https://www.linkedin.com/company/equilibrium-energy-inc\n",
      "    ⚠ Could not find website (LinkedIn found)\n",
      "\n",
      "[107/173] Searching for: Iberdrola\n",
      "    ✓ Website already exists: https://www.iberdrola.com › ...\n",
      "    → Searching for LinkedIn...\n",
      "    Search query: Iberdrola accelerating americas linkedin\n",
      "    Found LinkedIn: https://in.linkedin.com/company/iberdrola/jobs\n",
      "    ✓ LinkedIn saved: https://in.linkedin.com/company/iberdrola/jobs\n",
      "    ⚠ Could not find website (LinkedIn found)\n",
      "\n",
      "[108/173] Searching for: EsVolta\n",
      "    ✓ Website already exists: https://www.esvolta.com\n",
      "    → Searching for LinkedIn...\n",
      "    Search query: EsVolta empowering grid linkedin\n",
      "    Found LinkedIn: https://www.linkedin.com/company/esvolta-lp\n",
      "    ✓ LinkedIn saved: https://www.linkedin.com/company/esvolta-lp\n",
      "    ⚠ Could not find website (LinkedIn found)\n",
      "\n",
      "--- Progress saved: 0 websites found ---\n",
      "\n",
      "============================================================\n",
      "BATCH COMPLETE!\n",
      "Processed: 8 companies\n",
      "Websites found: 0\n",
      "Failed: 0\n",
      "\n",
      "⚠️  65 companies remaining.\n",
      "To continue: Update START_INDEX = 108\n",
      "\n",
      "✓ Results saved to: GFI_Data_Updated.csv\n",
      "\n",
      "To continue processing:\n",
      "  1. Update START_INDEX = 108\n",
      "  2. Run the script again\n",
      "\n",
      "Closing browser...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "'''Extracts WebsitE URL and linkedin URL for all the companies'''\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import time\n",
    "import requests\n",
    "\n",
    "class WebsiteFinderSelenium:\n",
    "    def __init__(self, headless=False):\n",
    "        \"\"\"Initialize Chrome driver with anti-detection measures\"\"\"\n",
    "        chrome_options = Options()\n",
    "        if headless:\n",
    "            chrome_options.add_argument(\"--headless\")\n",
    "        \n",
    "        # Anti-detection measures\n",
    "        chrome_options.add_argument(\"--no-sandbox\")\n",
    "        chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        chrome_options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "        chrome_options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "        chrome_options.add_experimental_option('useAutomationExtension', False)\n",
    "        \n",
    "        # Add realistic user agent\n",
    "        chrome_options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\")\n",
    "        \n",
    "        self.driver = webdriver.Chrome(options=chrome_options)\n",
    "        \n",
    "        # Hide webdriver property\n",
    "        self.driver.execute_cdp_cmd(\"Page.addScriptToEvaluateOnNewDocument\", {\n",
    "            \"source\": \"\"\"\n",
    "                Object.defineProperty(navigator, 'webdriver', {\n",
    "                    get: () => undefined\n",
    "                })\n",
    "            \"\"\"\n",
    "        })\n",
    "        \n",
    "        self.wait = WebDriverWait(self.driver, 10)\n",
    "        self.search_count = 0\n",
    "    \n",
    "    def clean_url(self, url: str) -> str:\n",
    "        \"\"\"Clean and normalize URLs\"\"\"\n",
    "        if not url:\n",
    "            return None\n",
    "        \n",
    "        url = url.strip()\n",
    "        \n",
    "        # Remove common prefixes we don't want\n",
    "        if url.startswith('›'):\n",
    "            url = url[1:].strip()\n",
    "        \n",
    "        # Ensure https://\n",
    "        if not url.startswith('http'):\n",
    "            url = 'https://' + url\n",
    "        \n",
    "        # Remove trailing slashes\n",
    "        url = url.rstrip('/')\n",
    "        \n",
    "        return url\n",
    "    \n",
    "    def validate_url(self, url: str) -> bool:\n",
    "        \"\"\"Quick check if URL is accessible\"\"\"\n",
    "        try:\n",
    "            response = requests.head(url, timeout=5, allow_redirects=True)\n",
    "            return response.status_code < 400\n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "    def extract_keywords(self, description: str, max_words: int = 3) -> str:\n",
    "        \"\"\"Extract key descriptive words from company description\"\"\"\n",
    "        if not description:\n",
    "            return \"\"\n",
    "        \n",
    "        # Common words to skip\n",
    "        skip_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', \n",
    "                     'of', 'with', 'by', 'from', 'up', 'about', 'into', 'through', 'during',\n",
    "                     'including', 'our', 'we', 'your', 'their', 'is', 'are', 'was', 'were'}\n",
    "        \n",
    "        # Split and clean words\n",
    "        words = description.lower().split()\n",
    "        keywords = []\n",
    "        \n",
    "        for word in words:\n",
    "            # Remove punctuation\n",
    "            word = ''.join(c for c in word if c.isalnum())\n",
    "            # Keep meaningful words\n",
    "            if word and word not in skip_words and len(word) > 3:\n",
    "                keywords.append(word)\n",
    "                if len(keywords) >= max_words:\n",
    "                    break\n",
    "        \n",
    "        return ' '.join(keywords)\n",
    "    \n",
    "    def search_google(self, company_name: str, description: str = \"\", search_type: str = \"website\") -> str:\n",
    "        \"\"\"Search Google using Selenium with company description\n",
    "        \n",
    "        Args:\n",
    "            company_name: Name of the company\n",
    "            description: Company description for context\n",
    "            search_type: Either \"website\" or \"linkedin\"\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Go to Google\n",
    "            self.driver.get(\"https://www.google.com\")\n",
    "            time.sleep(2)  # Increased delay\n",
    "            \n",
    "            # Accept cookies if popup appears\n",
    "            try:\n",
    "                accept_button = self.driver.find_element(By.XPATH, \"//button[contains(., 'Accept') or contains(., 'I agree')]\")\n",
    "                accept_button.click()\n",
    "                time.sleep(2)\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            # Build search query based on search type\n",
    "            if search_type == \"linkedin\":\n",
    "                search_query = f\"{company_name} linkedin\"\n",
    "                if description:\n",
    "                    keywords = self.extract_keywords(description, max_words=2)\n",
    "                    if keywords:\n",
    "                        search_query = f\"{company_name} {keywords} linkedin\"\n",
    "            else:  # website\n",
    "                search_query = f\"{company_name} official website\"\n",
    "                if description:\n",
    "                    keywords = self.extract_keywords(description, max_words=3)\n",
    "                    if keywords:\n",
    "                        search_query = f\"{company_name} {keywords} official website\"\n",
    "            \n",
    "            print(f\"    Search query: {search_query}\")\n",
    "            \n",
    "            # Find search box and enter query\n",
    "            search_box = self.driver.find_element(By.NAME, \"q\")\n",
    "            search_box.send_keys(search_query)\n",
    "            search_box.send_keys(Keys.RETURN)\n",
    "            \n",
    "            # Wait for results\n",
    "            time.sleep(3)\n",
    "            \n",
    "            # Try to find the result URL based on search type\n",
    "            try:\n",
    "                if search_type == \"linkedin\":\n",
    "                    # Look specifically for LinkedIn URLs\n",
    "                    links = self.driver.find_elements(By.CSS_SELECTOR, \"a[href*='linkedin.com/company']\")\n",
    "                    if links:\n",
    "                        href = links[0].get_attribute('href')\n",
    "                        # Clean LinkedIn URL (remove tracking parameters)\n",
    "                        if '?' in href:\n",
    "                            href = href.split('?')[0]\n",
    "                        print(f\"    Found LinkedIn: {href}\")\n",
    "                        return href\n",
    "                    \n",
    "                    # Fallback: check all links for linkedin\n",
    "                    all_links = self.driver.find_elements(By.CSS_SELECTOR, \"#search a\")\n",
    "                    for link in all_links[:10]:\n",
    "                        href = link.get_attribute('href')\n",
    "                        if href and 'linkedin.com/company' in href:\n",
    "                            if '?' in href:\n",
    "                                href = href.split('?')[0]\n",
    "                            print(f\"    Found LinkedIn via fallback: {href}\")\n",
    "                            return href\n",
    "                \n",
    "                else:  # website search\n",
    "                    # Method 1: Look for cite elements\n",
    "                    cite_elements = self.driver.find_elements(By.TAG_NAME, \"cite\")\n",
    "                    for cite in cite_elements:\n",
    "                        url_text = cite.text.strip()\n",
    "                        if url_text and 'google.com' not in url_text.lower():\n",
    "                            print(f\"    Found via cite: {url_text}\")\n",
    "                            return self.clean_url(url_text)\n",
    "                    \n",
    "                    # Method 2: Look for result links\n",
    "                    results = self.driver.find_elements(By.CSS_SELECTOR, \"div.yuRUbf a\")\n",
    "                    if results:\n",
    "                        href = results[0].get_attribute('href')\n",
    "                        if href and 'google.com' not in href:\n",
    "                            print(f\"    Found via link: {href}\")\n",
    "                            return href\n",
    "                    \n",
    "                    # Method 3: Look for any visible link in results\n",
    "                    links = self.driver.find_elements(By.CSS_SELECTOR, \"#search a\")\n",
    "                    for link in links[:5]:\n",
    "                        href = link.get_attribute('href')\n",
    "                        if href and href.startswith('http') and 'google.com' not in href and 'youtube.com' not in href:\n",
    "                            print(f\"    Found via search link: {href}\")\n",
    "                            return href\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"    Error extracting result: {e}\")\n",
    "            \n",
    "            return None\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"    Search error: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def process_csv(self, input_file: str, output_file: str, start_index: int = 0, batch_size: int = 20):\n",
    "        \"\"\"Process CSV file and find missing website URLs\"\"\"\n",
    "        # Read CSV - use output file if it exists (to preserve previous work)\n",
    "        import os\n",
    "        if os.path.exists(output_file) and start_index > 0:\n",
    "            print(f\"Loading previous progress from: {output_file}\")\n",
    "            df = pd.read_csv(output_file)\n",
    "        else:\n",
    "            print(f\"Loading original data from: {input_file}\")\n",
    "            df = pd.read_csv(input_file)\n",
    "        \n",
    "        # Process only a batch\n",
    "        end_index = min(start_index + batch_size, len(df))\n",
    "        \n",
    "        print(f\"Processing companies {start_index+1} to {end_index} (Total: {len(df)})...\")\n",
    "        \n",
    "        # Track progress\n",
    "        found_count = 0\n",
    "        failed_companies = []\n",
    "        \n",
    "        for idx in range(start_index, end_index):\n",
    "            row = df.iloc[idx]\n",
    "            company_name = row['Company Name']\n",
    "            website = row.get('Website URL', '')\n",
    "            linkedin_url = row.get('Linkedin URL', '')\n",
    "            description = row.get('Company Description', '')\n",
    "            \n",
    "            # Check if both website and LinkedIn already exist\n",
    "            has_website = pd.notna(website) and str(website).strip() and str(website).strip() != 'nan'\n",
    "            has_linkedin = pd.notna(linkedin_url) and str(linkedin_url).strip() and str(linkedin_url).strip() != 'nan'\n",
    "            \n",
    "            # Skip only if BOTH exist\n",
    "            if has_website and has_linkedin:\n",
    "                print(f\"[{idx+1}/{len(df)}] {company_name}: ✓ Already has both website and LinkedIn\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"\\n[{idx+1}/{len(df)}] Searching for: {company_name}\")\n",
    "            \n",
    "            # Search for website if missing\n",
    "            found_website = None\n",
    "            if not has_website:\n",
    "                print(f\"    → Searching for website...\")\n",
    "                found_website = self.search_google(company_name, description, search_type=\"website\")\n",
    "                time.sleep(3)  # Delay between searches\n",
    "            else:\n",
    "                print(f\"    ✓ Website already exists: {website}\")\n",
    "            \n",
    "            # Search for LinkedIn URL if missing\n",
    "            found_linkedin = None\n",
    "            if not has_linkedin:\n",
    "                print(f\"    → Searching for LinkedIn...\")\n",
    "                found_linkedin = self.search_google(company_name, description, search_type=\"linkedin\")\n",
    "                time.sleep(3)  # Delay between searches\n",
    "            else:\n",
    "                print(f\"    ✓ LinkedIn already exists: {linkedin_url}\")\n",
    "            \n",
    "            # Process website results\n",
    "            if found_website:\n",
    "                # Validate URL\n",
    "                print(f\"    Validating website...\")\n",
    "                if self.validate_url(found_website):\n",
    "                    df.at[idx, 'Website URL'] = found_website\n",
    "                    found_count += 1\n",
    "                    print(f\"    ✓ Website saved: {found_website}\")\n",
    "                else:\n",
    "                    # Try adding www\n",
    "                    alt_url = found_website.replace('https://', 'https://www.')\n",
    "                    if self.validate_url(alt_url):\n",
    "                        df.at[idx, 'Website URL'] = alt_url\n",
    "                        found_count += 1\n",
    "                        print(f\"    ✓ Website saved: {alt_url}\")\n",
    "                    else:\n",
    "                        # Save it anyway, might just be slow to respond\n",
    "                        df.at[idx, 'Website URL'] = found_website\n",
    "                        found_count += 1\n",
    "                        print(f\"    ⚠ Website saved (couldn't validate): {found_website}\")\n",
    "            \n",
    "            # Process LinkedIn results\n",
    "            if found_linkedin:\n",
    "                df.at[idx, 'Linkedin URL'] = found_linkedin\n",
    "                print(f\"    ✓ LinkedIn saved: {found_linkedin}\")\n",
    "            \n",
    "            # Track failures\n",
    "            if not found_website and not found_linkedin:\n",
    "                print(f\"    ✗ Could not find website or LinkedIn\")\n",
    "                failed_companies.append(company_name)\n",
    "            elif not found_website:\n",
    "                print(f\"    ⚠ Could not find website (LinkedIn found)\")\n",
    "            elif not found_linkedin:\n",
    "                print(f\"    ⚠ Could not find LinkedIn (Website found)\")\n",
    "            \n",
    "            # Save progress every 3 companies\n",
    "            if (idx + 1) % 3 == 0:\n",
    "                df.to_csv(output_file, index=False)\n",
    "                print(f\"\\n--- Progress saved: {found_count} websites found ---\")\n",
    "            \n",
    "            # Delay between companies (increased to avoid detection)\n",
    "            time.sleep(8)  # Increased from 4 to 8 seconds\n",
    "        \n",
    "        # Final save\n",
    "        df.to_csv(output_file, index=False)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(f\"BATCH COMPLETE!\")\n",
    "        print(f\"Processed: {end_index - start_index} companies\")\n",
    "        print(f\"Websites found: {found_count}\")\n",
    "        print(f\"Failed: {len(failed_companies)}\")\n",
    "        \n",
    "        if failed_companies:\n",
    "            print(\"\\nFailed companies:\")\n",
    "            for company in failed_companies[:10]:\n",
    "                print(f\"  - {company}\")\n",
    "        \n",
    "        if end_index < len(df):\n",
    "            print(f\"\\n⚠️  {len(df) - end_index} companies remaining.\")\n",
    "            print(f\"To continue: Update START_INDEX = {end_index}\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Close the browser\"\"\"\n",
    "        self.driver.quit()\n",
    "\n",
    "\n",
    "# USAGE EXAMPLE\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Initializing Chrome browser...\")\n",
    "    print(\"A Chrome window will open - don't close it!\\n\")\n",
    "    \n",
    "    finder = WebsiteFinderSelenium(headless=False)  # Set True to hide browser\n",
    "    \n",
    "    try:\n",
    "        input_file = \"Growth For Impact Data Assignment_Data.csv\"\n",
    "        output_file = \"GFI_Data_Updated.csv\"\n",
    "        \n",
    "        # Configure batch processing\n",
    "        START_INDEX = 100   # Change to continue from where you left off\n",
    "        BATCH_SIZE = 8   # Reduced to 10 to avoid CAPTCHA (was 20)\n",
    "        \n",
    "        print(\"Starting website URL finder with Selenium...\")\n",
    "        print(\"This uses real Chrome browser - more reliable!\\n\")\n",
    "        \n",
    "        df_updated = finder.process_csv(\n",
    "            input_file, \n",
    "            output_file,\n",
    "            start_index=START_INDEX,\n",
    "            batch_size=BATCH_SIZE\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n✓ Results saved to: {output_file}\")\n",
    "        print(\"\\nTo continue processing:\")\n",
    "        print(f\"  1. Update START_INDEX = {START_INDEX + BATCH_SIZE}\")\n",
    "        print(f\"  2. Run the script again\")\n",
    "        \n",
    "    finally:\n",
    "        print(\"\\nClosing browser...\")\n",
    "        finder.close()\n",
    "        print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "51f61075-934f-4951-a9ae-1b06ff777dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Career Page Finder...\n",
      "\n",
      "Loading previous progress from: GFI_Data_With_Careers.csv\n",
      "Processing companies 148 to 172 (Total: 173)...\n",
      "\n",
      "\n",
      "[148/173] Processing: Context Labs\n",
      "      Crawling website: https://contextlabs.com\n",
      "      ✓ Found career page: https://contextlabs.com/company/careers/\n",
      "      → Following link: 'explore open positions' to https://contextlabs.bamboohr.com/jobs/\n",
      "      ✓ Found job listing page (redirect): https://contextlabs.bamboohr.com/careers\n",
      "      ✓ Career page saved\n",
      "      ✓ Job listing page saved\n",
      "\n",
      "[149/173] Processing: 350.org\n",
      "      Crawling website: https://350.org\n",
      "      ✗ No career page found\n",
      "      ✗ No career page found on company website - leaving blank\n",
      "      ✗ No career/job pages found - leaving blank\n",
      "\n",
      "[150/173] Processing: Scope3\n",
      "      Crawling website: https://scope3.com › blog › category › company-news\n",
      "      Error crawling website: Message: unknown error: net::ERR_NAME_NOT_RESOLVED\n",
      "  (Session info: chrome=142.0.7444.60)\n",
      "Stacktrace:\n",
      "Symbols not available. Dumping unresolved backtrace:\n",
      "\t0x7ff650277a05\n",
      "\t0x7ff650277a60\n",
      "\t0x7ff64fff16ad\n",
      "\t0x7ff64ffee1f8\n",
      "\t0x7ff64ffdeb69\n",
      "\t0x7ff64ffe0997\n",
      "\t0x7ff64ffdf120\n",
      "\t0x7ff64ffde8d7\n",
      "\t0x7ff64ffde59b\n",
      "\t0x7ff64ffdc105\n",
      "\t0x7ff64ffdc99c\n",
      "\t0x7ff64fff59ba\n",
      "\t0x7ff65009c72e\n",
      "\t0x7ff65007297a\n",
      "\t0x7ff65009b8fb\n",
      "\t0x7ff65003b068\n",
      "\t0x7ff65003be93\n",
      "\t0x7ff6505329a0\n",
      "\t0x7ff65052ce20\n",
      "\t0x7ff65054cc15\n",
      "\t0x7ff65029309e\n",
      "\t0x7ff65029ad8f\n",
      "\t0x7ff650280be4\n",
      "\t0x7ff650280d9f\n",
      "\t0x7ff6502667f8\n",
      "\t0x7ffc6e69e8d7\n",
      "\t0x7ffc7092c53c\n",
      "\n",
      "      ✗ No career page found on company website - leaving blank\n",
      "      ✗ No career/job pages found - leaving blank\n",
      "\n",
      "--- Progress saved: 1 career pages found ---\n",
      "\n",
      "[151/173] Processing: Generate Capital\n",
      "      Crawling website: https://generatecapital.com\n",
      "      ✓ Found career page: https://generatecapital.com/careers\n",
      "      ✓ Found job listing in iframe: https://jobs.ashbyhq.com/generate?embed=js\n",
      "      ✓ Career page saved\n",
      "      ✓ Job listing page saved\n",
      "\n",
      "[152/173] Processing: Hydrosat\n",
      "      Crawling website: https://hydrosat.com\n",
      "      ✓ Found career page: http://careers.hydrosat.com/\n",
      "      ✓ Found job listing page: https://recruitee.com/\n",
      "      ✓ Career page saved\n",
      "      ✓ Job listing page saved\n",
      "\n",
      "[153/173] Processing: Phaidra\n",
      "      Crawling website: https://www.phaidra.ai\n",
      "      ✓ Found career page: https://www.phaidra.ai/careers\n",
      "      ✓ Using career page as job listing page (fallback)\n",
      "      ✓ Career page saved\n",
      "      ✓ Job listing page saved\n",
      "\n",
      "--- Progress saved: 4 career pages found ---\n",
      "\n",
      "[154/173] Processing: Charge Point\n",
      "      Crawling website: https://www.chargepoint.com\n",
      "      ✓ Found career page: https://www.chargepoint.com/about/opportunities\n",
      "      ✓ Using career page as job listing page (fallback)\n",
      "      ✓ Career page saved\n",
      "      ✓ Job listing page saved\n",
      "\n",
      "[155/173] Processing: Unison Infrastructure\n",
      "      Crawling website: https://unisoninfra.com\n",
      "      ✓ Found career page: https://unisoninfra.com/careers/\n",
      "      ✓ Found job listing page: https://jobs.lever.co/unisoninfra/c15a461b-d1c1-48f9-9500-338d3406a5c7\n",
      "      ✓ Career page saved\n",
      "      ✓ Job listing page saved\n",
      "\n",
      "[156/173] Processing: Last Energy\n",
      "      Crawling website: https://www.lastenergy.com\n",
      "      ✓ Found job platform: https://jobs.lever.co/last-energy\n",
      "      ✓ Career page is on job platform\n",
      "      ✓ Career page saved\n",
      "      ✓ Job listing page saved\n",
      "\n",
      "--- Progress saved: 7 career pages found ---\n",
      "\n",
      "[157/173] Processing: Velo3D\n",
      "      Crawling website: https://velo3d.com\n",
      "      ✗ No career page found\n",
      "      ✗ No career page found on company website - leaving blank\n",
      "      ✗ No career/job pages found - leaving blank\n",
      "\n",
      "[158/173] Processing: Sublime Systems\n",
      "      Crawling website: https://sublime-systems.com\n",
      "      ✓ Found job platform: https://jobs.lever.co/SublimeSystems\n",
      "      ✓ Career page is on job platform\n",
      "      ✓ Career page saved\n",
      "      ✓ Job listing page saved\n",
      "\n",
      "[159/173] Processing: Brookfield Global Asset Management Limited\n",
      "      Crawling website: https://bam.brookfield.com\n",
      "      ✓ Found career page: https://www.brookfield.com/careers\n",
      "      ✓ Using career page as job listing page (fallback)\n",
      "      ✓ Career page saved\n",
      "      ✓ Job listing page saved\n",
      "\n",
      "--- Progress saved: 9 career pages found ---\n",
      "\n",
      "[160/173] Processing: Muon Space\n",
      "      Crawling website: https://www.muonspace.com\n",
      "      ✓ Found career page: https://www.muonspace.com/careers\n",
      "      ✓ Found job listing page: https://boards.greenhouse.io/muonspace\n",
      "      ✓ Career page saved\n",
      "      ✓ Job listing page saved\n",
      "\n",
      "[161/173] Processing: BorgWarner\n",
      "      Crawling website: https://www.borgwarner.com › company › sustainability\n",
      "      Error crawling website: Message: unknown error: net::ERR_NAME_NOT_RESOLVED\n",
      "  (Session info: chrome=142.0.7444.60)\n",
      "Stacktrace:\n",
      "Symbols not available. Dumping unresolved backtrace:\n",
      "\t0x7ff650277a05\n",
      "\t0x7ff650277a60\n",
      "\t0x7ff64fff16ad\n",
      "\t0x7ff64ffee1f8\n",
      "\t0x7ff64ffdeb69\n",
      "\t0x7ff64ffe0997\n",
      "\t0x7ff64ffdf120\n",
      "\t0x7ff64ffde8d7\n",
      "\t0x7ff64ffde59b\n",
      "\t0x7ff64ffdc105\n",
      "\t0x7ff64ffdc99c\n",
      "\t0x7ff64fff59ba\n",
      "\t0x7ff65009c72e\n",
      "\t0x7ff65007297a\n",
      "\t0x7ff65009b8fb\n",
      "\t0x7ff65003b068\n",
      "\t0x7ff65003be93\n",
      "\t0x7ff6505329a0\n",
      "\t0x7ff65052ce20\n",
      "\t0x7ff65054cc15\n",
      "\t0x7ff65029309e\n",
      "\t0x7ff65029ad8f\n",
      "\t0x7ff650280be4\n",
      "\t0x7ff650280d9f\n",
      "\t0x7ff6502667f8\n",
      "\t0x7ffc6e69e8d7\n",
      "\t0x7ffc7092c53c\n",
      "\n",
      "      ✗ No career page found on company website - leaving blank\n",
      "      ✗ No career/job pages found - leaving blank\n",
      "\n",
      "[162/173] Processing: Fluence\n",
      "      Crawling website: https://fluenceenergy.com\n",
      "      ✗ No career page found\n",
      "      ✗ No career page found on company website - leaving blank\n",
      "      ✗ No career/job pages found - leaving blank\n",
      "\n",
      "--- Progress saved: 10 career pages found ---\n",
      "\n",
      "[163/173] Processing: Bidgely\n",
      "      Crawling website: https://www.bidgely.com\n",
      "      ✓ Found career page: https://www.bidgely.com/careers/\n",
      "      ✓ Using career page as job listing page (fallback)\n",
      "      ✓ Career page saved\n",
      "      ✓ Job listing page saved\n",
      "\n",
      "[164/173] Processing: XENERGY\n",
      "      Crawling website: https://www.x-energy.com\n",
      "      ✓ Found career page: https://x-energy.com/media/news-releases/centrica-and-x-energy-sign-joint-development-agreement-to-deploy-uks-first-advanced-modular-reactors\n",
      "      ✓ Using career page as job listing page (fallback)\n",
      "      ✓ Career page saved\n",
      "      ✓ Job listing page saved\n",
      "\n",
      "[165/173] Processing: Key Capture Energy\n",
      "      Crawling website: https://keycaptureenergy.com\n",
      "      ✓ Found career page: https://keycaptureenergy.com/careers/\n",
      "      ✓ Using career page as job listing page (fallback)\n",
      "      ✓ Career page saved\n",
      "      ✓ Job listing page saved\n",
      "\n",
      "--- Progress saved: 13 career pages found ---\n",
      "\n",
      "[166/173] Processing: Sky Climber Renewables\n",
      "      Crawling website: https://skyclimber-re.com\n",
      "      ✓ Found career page: https://skyclimber-re.com/careers/\n",
      "      → Following link: 'view open positions' to https://renewables-skyclimber.icims.com/jobs/intro\n",
      "      ✓ Found job listing page (redirect): https://renewables-skyclimber.icims.com/jobs/intro\n",
      "      ✓ Career page saved\n",
      "      ✓ Job listing page saved\n",
      "\n",
      "[167/173] Processing: World Wildlife Fund\n",
      "      Crawling website: https://www.worldwildlife.org\n",
      "      ✗ No career page found\n",
      "      ✗ No career page found on company website - leaving blank\n",
      "      ✗ No career/job pages found - leaving blank\n",
      "\n",
      "[168/173] Processing: Enpowered\n",
      "      Crawling website: https://enpowered.com\n",
      "      ✓ Found career page: https://enpowered.com/company/careers/\n",
      "      ✓ Using career page as job listing page (fallback)\n",
      "      ✓ Career page saved\n",
      "      ✓ Job listing page saved\n",
      "\n",
      "--- Progress saved: 15 career pages found ---\n",
      "\n",
      "[169/173] Processing: Power Factors\n",
      "      Crawling website: https://www.powerfactors.com\n",
      "      ✓ Found career page: https://www.powerfactors.com/careers\n",
      "      → Following link: 'see open positions' to https://www.powerfactors.com/careers#job_board_card_deck_workable\n",
      "      ✓ Found job listing page (redirect): https://www.powerfactors.com/careers#job_board_card_deck_workable\n",
      "      ✓ Career page saved\n",
      "      ✓ Job listing page saved\n",
      "\n",
      "[170/173] Processing: Ecology Project International\n",
      "      Crawling website: https://www.ecologyproject.org\n",
      "      ✓ Found career page: https://www.ecologyproject.org/employment\n",
      "      ✓ Using career page as job listing page (fallback)\n",
      "      ✓ Career page saved\n",
      "      ✓ Job listing page saved\n",
      "\n",
      "[171/173] Processing: Enverus\n",
      "      Crawling website: https://www.enverus.com\n",
      "      ✓ Found career page: https://www.enverus.com/careers/\n",
      "      ✓ Using career page as job listing page (fallback)\n",
      "      ✓ Career page saved\n",
      "      ✓ Job listing page saved\n",
      "\n",
      "--- Progress saved: 18 career pages found ---\n",
      "\n",
      "[172/173] Processing: FlexGen\n",
      "      Crawling website: https://www.flexgen.com\n",
      "      ✓ Found career page: https://www.flexgen.com/careers\n",
      "      ✓ Using career page as job listing page (fallback)\n",
      "      ✓ Career page saved\n",
      "      ✓ Job listing page saved\n",
      "\n",
      "============================================================\n",
      "BATCH COMPLETE!\n",
      "Processed: 25 companies\n",
      "Career pages found: 19\n",
      "\n",
      "⚠️  1 companies remaining.\n",
      "To continue: update START_INDEX = 172\n",
      "\n",
      "✓ Results saved to: GFI_Data_With_Careers.csv\n",
      "\n",
      "Closing browser...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "'''Extracts career page URL and job listings page URL'''\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "import time\n",
    "import requests\n",
    "from urllib.parse import urljoin\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import os\n",
    "\n",
    "\n",
    "class CareerPageFinder:\n",
    "    def __init__(self, headless=False):\n",
    "        \"\"\"Initialize Chrome driver\"\"\"\n",
    "        chrome_options = Options()\n",
    "        if headless:\n",
    "            chrome_options.add_argument(\"--headless\")\n",
    "        chrome_options.add_argument(\"--no-sandbox\")\n",
    "        chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        chrome_options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "        chrome_options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "        chrome_options.add_experimental_option(\"useAutomationExtension\", False)\n",
    "\n",
    "        # Initialize driver (make sure chromedriver is in PATH or use webdriver-manager)\n",
    "        self.driver = webdriver.Chrome(options=chrome_options)\n",
    "        # Hide webdriver flag\n",
    "        try:\n",
    "            self.driver.execute_cdp_cmd(\"Page.addScriptToEvaluateOnNewDocument\", {\n",
    "                \"source\": \"\"\"\n",
    "                    Object.defineProperty(navigator, 'webdriver', {\n",
    "                        get: () => undefined\n",
    "                    })\n",
    "                \"\"\"\n",
    "            })\n",
    "        except Exception:\n",
    "            # not critical if CDP not available\n",
    "            pass\n",
    "\n",
    "        self.wait = WebDriverWait(self.driver, 10)\n",
    "\n",
    "        # Known job board platforms and career keywords\n",
    "        self.job_platforms = [\n",
    "            'lever.co', 'greenhouse.io', 'workable.com', 'ashbyhq.com',\n",
    "            'jobs.lever.co', 'boards.greenhouse.io', 'apply.workable.com',\n",
    "            'zohorecruit.com', 'myworkdayjobs.com', 'successfactors.com',\n",
    "            'icims.com', 'taleo.net', 'breezy.hr', 'personio.de',\n",
    "            'personio.com', 'teamtailor.com', 'recruitee.com', 'bamboohr.com',\n",
    "            'smartrecruiters.com', 'jobvite.com', 'fountain.com'\n",
    "        ]\n",
    "\n",
    "    def is_valid_url(self, url):\n",
    "        \"\"\"Check if URL is valid and accessible\"\"\"\n",
    "        if not url or not isinstance(url, str):\n",
    "            return False\n",
    "        url = url.strip()\n",
    "        if not (url.startswith('http://') or url.startswith('https://')):\n",
    "            # try adding scheme\n",
    "            url = 'https://' + url\n",
    "        try:\n",
    "            response = requests.head(url, timeout=6, allow_redirects=True)\n",
    "            return response.status_code < 400\n",
    "        except Exception:\n",
    "            return False\n",
    "\n",
    "    def find_career_page_from_website(self, website_url):\n",
    "        \"\"\"Find career page by crawling the website\"\"\"\n",
    "        if not website_url or not isinstance(website_url, str):\n",
    "            return None, None\n",
    "\n",
    "        try:\n",
    "            if not website_url.startswith('http'):\n",
    "                website_url = 'https://' + website_url\n",
    "\n",
    "            print(f\"      Crawling website: {website_url}\")\n",
    "            self.driver.get(website_url)\n",
    "            time.sleep(2)\n",
    "\n",
    "            # Get all links on the page\n",
    "            links = self.driver.find_elements(By.TAG_NAME, \"a\")\n",
    "\n",
    "            career_candidates = []\n",
    "            job_platform_links = []\n",
    "\n",
    "            for link in links:\n",
    "                try:\n",
    "                    href = link.get_attribute('href')\n",
    "                    text = (link.text or '').lower().strip()\n",
    "                    if not href:\n",
    "                        continue\n",
    "                    href_lower = href.lower()\n",
    "\n",
    "                    # Priority 1: Known job platforms\n",
    "                    for platform in self.job_platforms:\n",
    "                        if platform in href_lower:\n",
    "                            job_platform_links.append((href, 10))\n",
    "                            break\n",
    "\n",
    "                    # Priority 2: Strong career keywords in URL\n",
    "                    strong_keywords = ['careers', 'jobs', 'join-us', 'hiring', 'work-with-us', 'work-at', 'join']\n",
    "                    for keyword in strong_keywords:\n",
    "                        if f'/{keyword}' in href_lower or f'-{keyword}' in href_lower or f'/{keyword}/' in href_lower:\n",
    "                            if any(word in text for word in ['career', 'job', 'join', 'work', 'hiring', 'opportunity']):\n",
    "                                career_candidates.append((href, 8))\n",
    "                            else:\n",
    "                                career_candidates.append((href, 6))\n",
    "                            break\n",
    "\n",
    "                    # Priority 3: Career keywords in link text\n",
    "                    if any(word in text for word in ['career', 'jobs', 'join us', 'work with us', 'opportunities', \"we're hiring\"]):\n",
    "                        if href not in [c[0] for c in career_candidates]:\n",
    "                            career_candidates.append((href, 5))\n",
    "                except Exception:\n",
    "                    continue\n",
    "\n",
    "            # First, check job platform links\n",
    "            if job_platform_links:\n",
    "                best_link = max(job_platform_links, key=lambda x: x[1])[0]\n",
    "                if self.is_valid_url(best_link):\n",
    "                    print(f\"      ✓ Found job platform: {best_link}\")\n",
    "                    jobs_listing = self.find_jobs_listing_page(best_link)\n",
    "                    return best_link, jobs_listing\n",
    "\n",
    "            # Then career candidates by priority\n",
    "            if career_candidates:\n",
    "                career_candidates.sort(key=lambda x: x[1], reverse=True)\n",
    "                for candidate_url, priority in career_candidates:\n",
    "                    if self.is_valid_url(candidate_url):\n",
    "                        print(f\"      ✓ Found career page: {candidate_url}\")\n",
    "                        jobs_listing = self.find_jobs_listing_page(candidate_url)\n",
    "                        return candidate_url, jobs_listing\n",
    "\n",
    "            # Try common patterns on base URL\n",
    "            base_url = website_url.rstrip('/')\n",
    "            common_paths = [\n",
    "                '/careers', '/jobs', '/careers/', '/jobs/',\n",
    "                '/work-with-us', '/join-us', '/join',\n",
    "                '/about/careers', '/company/careers', '/team/careers'\n",
    "            ]\n",
    "            for path in common_paths:\n",
    "                test_url = base_url + path\n",
    "                if self.is_valid_url(test_url):\n",
    "                    print(f\"      ✓ Found career page via pattern: {test_url}\")\n",
    "                    jobs_listing = self.find_jobs_listing_page(test_url)\n",
    "                    return test_url, jobs_listing\n",
    "\n",
    "            print(\"      ✗ No career page found\")\n",
    "            return None, None\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"      Error crawling website: {e}\")\n",
    "            return None, None\n",
    "\n",
    "    def find_jobs_listing_page(self, career_page_url):\n",
    "        \"\"\"Find the actual job listings page from career page\"\"\"\n",
    "        if not career_page_url:\n",
    "            return None\n",
    "\n",
    "        try:\n",
    "            # If on known job platform, try /jobs\n",
    "            if any(platform in career_page_url for platform in self.job_platforms):\n",
    "                print(\"      ✓ Career page is on job platform\")\n",
    "                base_platform_url = career_page_url.rstrip('/')\n",
    "                if '/jobs' not in base_platform_url:\n",
    "                    test_jobs_url = f\"{base_platform_url}/jobs\"\n",
    "                    if self.is_valid_url(test_jobs_url):\n",
    "                        print(f\"      ✓ Found /jobs page: {test_jobs_url}\")\n",
    "                        return test_jobs_url\n",
    "                return career_page_url\n",
    "\n",
    "            # Load the career page\n",
    "            if not career_page_url.startswith('http'):\n",
    "                career_page_url = 'https://' + career_page_url\n",
    "            self.driver.get(career_page_url)\n",
    "            time.sleep(2)\n",
    "            page_source = self.driver.page_source.lower()\n",
    "\n",
    "            exclude_keywords = ['privacy', 'cookie', 'terms', 'contact', 'faq', 'news', 'events', 'blog']\n",
    "\n",
    "            # Check for iframes (embedded job boards)\n",
    "            iframes = self.driver.find_elements(By.TAG_NAME, \"iframe\")\n",
    "            for iframe in iframes:\n",
    "                try:\n",
    "                    iframe_src = iframe.get_attribute('src') or ''\n",
    "                    if iframe_src and any(platform in iframe_src for platform in self.job_platforms):\n",
    "                        # avoid individual job postings (ending digits) and apply pages\n",
    "                        if not re.search(r'/\\d+$', iframe_src) and '/apply/' not in iframe_src.lower():\n",
    "                            print(f\"      ✓ Found job listing in iframe: {iframe_src}\")\n",
    "                            return iframe_src\n",
    "                except Exception:\n",
    "                    continue\n",
    "\n",
    "            # Look for links on the career page\n",
    "            links = self.driver.find_elements(By.TAG_NAME, \"a\")\n",
    "            job_platform_candidates = []\n",
    "            job_keyword_candidates = []\n",
    "            redirect_candidates = []\n",
    "\n",
    "            for link in links:\n",
    "                try:\n",
    "                    href = link.get_attribute('href')\n",
    "                    text = (link.text or '').lower().strip()\n",
    "                    if not href:\n",
    "                        continue\n",
    "                    href_lower = href.lower()\n",
    "\n",
    "                    # Skip excluded pages\n",
    "                    if any(excl in href_lower for excl in exclude_keywords):\n",
    "                        continue\n",
    "\n",
    "                    # Skip individual job postings (patterns like /1234, /job/123, /apply/123, /j/ABC)\n",
    "                    if (\n",
    "                        re.search(r'/\\d+$', href_lower) or\n",
    "                        '/apply/' in href_lower or\n",
    "                        re.search(r'/job/\\d+', href_lower) or\n",
    "                        re.search(r'/j/[A-Za-z0-9\\-]+', href_lower)\n",
    "                    ):\n",
    "                        continue\n",
    "\n",
    "                    # Priority 1: Job platforms\n",
    "                    for platform in self.job_platforms:\n",
    "                        if platform in href_lower:\n",
    "                            if '/jobs' in href_lower:\n",
    "                                job_platform_candidates.append((href, 10))\n",
    "                            else:\n",
    "                                job_platform_candidates.append((href, 7))\n",
    "                            break\n",
    "\n",
    "                    # Priority 2: Strong job keywords in link text\n",
    "                    strong_job_keywords = [\n",
    "                        'job openings', 'view jobs', 'see jobs', 'open positions',\n",
    "                        'view openings', 'see open roles', 'current openings', 'browse jobs'\n",
    "                    ]\n",
    "                    if any(keyword in text for keyword in strong_job_keywords):\n",
    "                        redirect_candidates.append((href, text, 9))\n",
    "\n",
    "                    # Priority 3: Generic job keywords\n",
    "                    job_keywords = ['jobs', 'openings', 'positions', 'apply now', 'vacancies']\n",
    "                    if any(keyword in text for keyword in job_keywords):\n",
    "                        if '/jobs' in href_lower or '/careers' in href_lower or any(platform in href_lower for platform in self.job_platforms):\n",
    "                            job_keyword_candidates.append((href, 8))\n",
    "                except Exception:\n",
    "                    continue\n",
    "\n",
    "            # Follow redirect candidates first\n",
    "            if redirect_candidates:\n",
    "                redirect_candidates.sort(key=lambda x: x[2], reverse=True)\n",
    "                for candidate_url, link_text, priority in redirect_candidates[:3]:\n",
    "                    try:\n",
    "                        print(f\"      → Following link: '{link_text}' to {candidate_url}\")\n",
    "                        self.driver.get(candidate_url)\n",
    "                        time.sleep(2)\n",
    "                        final_url = self.driver.current_url or candidate_url\n",
    "\n",
    "                        if any(platform in final_url for platform in self.job_platforms):\n",
    "                            if '/jobs' not in final_url:\n",
    "                                base_url = final_url.rstrip('/')\n",
    "                                test_jobs_url = f\"{base_url}/jobs\"\n",
    "                                if self.is_valid_url(test_jobs_url):\n",
    "                                    print(f\"      ✓ Found job listing page (redirect): {test_jobs_url}\")\n",
    "                                    return test_jobs_url\n",
    "                            print(f\"      ✓ Found job listing page (redirect): {final_url}\")\n",
    "                            return final_url\n",
    "\n",
    "                        page_content = self.driver.page_source.lower()\n",
    "                        if any(kw in page_content for kw in ['job title', 'job description', 'apply now', 'open position']):\n",
    "                            print(f\"      ✓ Found job listing page (redirect): {final_url}\")\n",
    "                            return final_url\n",
    "                    except Exception:\n",
    "                        continue\n",
    "\n",
    "            # If job platform candidates exist, prefer them\n",
    "            if job_platform_candidates:\n",
    "                best = max(job_platform_candidates, key=lambda x: x[1])[0]\n",
    "                if any(platform in best for platform in self.job_platforms) and '/jobs' not in best:\n",
    "                    base_url = best.rstrip('/')\n",
    "                    test_jobs_url = f\"{base_url}/jobs\"\n",
    "                    if self.is_valid_url(test_jobs_url):\n",
    "                        print(f\"      ✓ Found job listing page: {test_jobs_url}\")\n",
    "                        return test_jobs_url\n",
    "                if self.is_valid_url(best):\n",
    "                    print(f\"      ✓ Found job listing page: {best}\")\n",
    "                    return best\n",
    "\n",
    "            # If keyword candidates exist, return best\n",
    "            if job_keyword_candidates:\n",
    "                best = max(job_keyword_candidates, key=lambda x: x[1])[0]\n",
    "                if self.is_valid_url(best):\n",
    "                    print(f\"      ✓ Found job listing page: {best}\")\n",
    "                    return best\n",
    "\n",
    "            # Heuristic: look for job-related content on the career page itself\n",
    "            page_lower = page_source\n",
    "            job_indicators = 0\n",
    "            if 'apply now' in page_lower:\n",
    "                job_indicators += 1\n",
    "            if 'job title' in page_lower or 'position title' in page_lower:\n",
    "                job_indicators += 2\n",
    "            if 'job description' in page_lower:\n",
    "                job_indicators += 1\n",
    "            if 'open position' in page_lower or 'current opening' in page_lower:\n",
    "                job_indicators += 1\n",
    "\n",
    "            if job_indicators >= 2:\n",
    "                print(f\"      ✓ Career page contains job listings (indicators: {job_indicators})\")\n",
    "                return career_page_url\n",
    "\n",
    "            # If on job platform but no /jobs found, try to append /jobs\n",
    "            if any(platform in career_page_url for platform in self.job_platforms):\n",
    "                base_url = career_page_url.rstrip('/')\n",
    "                if '/jobs' not in base_url:\n",
    "                    test_jobs_url = f\"{base_url}/jobs\"\n",
    "                    if self.is_valid_url(test_jobs_url):\n",
    "                        print(f\"      ✓ Found job listing page (platform /jobs): {test_jobs_url}\")\n",
    "                        return test_jobs_url\n",
    "                print(\"      ✓ Using career page as job listing page (on job platform)\")\n",
    "                return career_page_url\n",
    "\n",
    "            # Fallback: return career page\n",
    "            print(\"      ✓ Using career page as job listing page (fallback)\")\n",
    "            return career_page_url\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"      Error finding job listing page: {e}\")\n",
    "            return None\n",
    "\n",
    "    def process_csv(self, input_file, output_file, start_index=0, batch_size=10):\n",
    "        \"\"\"Process CSV and find career pages and job listings\"\"\"\n",
    "        # Load data (resume if output exists and start_index > 0)\n",
    "        if os.path.exists(output_file) and start_index > 0:\n",
    "            print(f\"Loading previous progress from: {output_file}\")\n",
    "            df = pd.read_csv(output_file)\n",
    "        else:\n",
    "            print(f\"Loading data from: {input_file}\")\n",
    "            df = pd.read_csv(input_file)\n",
    "\n",
    "        # Ensure columns exist\n",
    "        if 'Careers Page URL' not in df.columns:\n",
    "            df['Careers Page URL'] = ''\n",
    "        if 'Job listings page URL' not in df.columns:\n",
    "            df['Job listings page URL'] = ''\n",
    "\n",
    "        # Normalize types\n",
    "        df['Careers Page URL'] = df['Careers Page URL'].astype(str)\n",
    "        df['Job listings page URL'] = df['Job listings page URL'].astype(str)\n",
    "\n",
    "        end_index = min(start_index + batch_size, len(df))\n",
    "        print(f\"Processing companies {start_index+1} to {end_index} (Total: {len(df)})...\\n\")\n",
    "\n",
    "        found_count = 0\n",
    "\n",
    "        for idx in range(start_index, end_index):\n",
    "            row = df.iloc[idx]\n",
    "            company_name = row.get('Company Name', f'Row {idx}')\n",
    "            website_url = row.get('Website URL', '') or ''\n",
    "            career_page = row.get('Careers Page URL', '')\n",
    "            job_listing = row.get('Job listings page URL', '')\n",
    "\n",
    "            # Skip if already filled\n",
    "            has_career = pd.notna(career_page) and str(career_page).strip() not in ['', 'nan']\n",
    "            has_listing = pd.notna(job_listing) and str(job_listing).strip() not in ['', 'nan']\n",
    "\n",
    "            if has_career and has_listing:\n",
    "                print(f\"[{idx+1}/{len(df)}] {company_name}: ✓ Already has career and listing pages\")\n",
    "                continue\n",
    "\n",
    "            print(f\"\\n[{idx+1}/{len(df)}] Processing: {company_name}\")\n",
    "\n",
    "            found_career = None\n",
    "            found_listing = None\n",
    "\n",
    "            # Try website only\n",
    "            if pd.notna(website_url) and str(website_url).strip():\n",
    "                found_career, found_listing = self.find_career_page_from_website(str(website_url).strip())\n",
    "\n",
    "            if not found_career:\n",
    "                print(\"      ✗ No career page found on company website - leaving blank\")\n",
    "\n",
    "            # Save results\n",
    "            if found_career:\n",
    "                df.at[idx, 'Careers Page URL'] = str(found_career)\n",
    "                found_count += 1\n",
    "                print(\"      ✓ Career page saved\")\n",
    "                if found_listing:\n",
    "                    df.at[idx, 'Job listings page URL'] = str(found_listing)\n",
    "                    print(\"      ✓ Job listing page saved\")\n",
    "            else:\n",
    "                print(\"      ✗ No career/job pages found - leaving blank\")\n",
    "\n",
    "            # Periodic save\n",
    "            if (idx + 1) % 3 == 0:\n",
    "                df.to_csv(output_file, index=False)\n",
    "                print(f\"\\n--- Progress saved: {found_count} career pages found ---\")\n",
    "\n",
    "            # polite delay\n",
    "            time.sleep(3)\n",
    "\n",
    "        # Final save\n",
    "        df.to_csv(output_file, index=False)\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(f\"BATCH COMPLETE!\\nProcessed: {end_index - start_index} companies\\nCareer pages found: {found_count}\")\n",
    "        if end_index < len(df):\n",
    "            print(f\"\\n⚠️  {len(df) - end_index} companies remaining.\\nTo continue: update START_INDEX = {end_index}\")\n",
    "\n",
    "        return df\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"Close browser\"\"\"\n",
    "        try:\n",
    "            self.driver.quit()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "\n",
    "# === USAGE EXAMPLE ===\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting Career Page Finder...\\n\")\n",
    "\n",
    "    finder = CareerPageFinder(headless=False)\n",
    "\n",
    "    try:\n",
    "        input_file = \"gfi_data_updated.csv\"      # your input CSV\n",
    "        output_file = \"GFI_Data_With_Careers.csv\"  # output CSV to save progress\n",
    "\n",
    "        START_INDEX = 147   # change to resume\n",
    "        BATCH_SIZE = 25   # how many rows to process in this run\n",
    "\n",
    "        df = finder.process_csv(input_file, output_file, START_INDEX, BATCH_SIZE)\n",
    "        print(f\"\\n✓ Results saved to: {output_file}\")\n",
    "\n",
    "    finally:\n",
    "        print(\"\\nClosing browser...\")\n",
    "        finder.close()\n",
    "        print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb6bdd7-5abc-42ac-946b-279a0fc5be5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "42b1fe82-0a58-4e86-96fe-4404818f8098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensure you have run: !pip install pandas selenium webdriver-manager\n",
      "Found existing output file. Loading from: GFI_Data_With_Jobs_Scraped_1.csv\n",
      "Loaded 173 rows from GFI_Data_With_Jobs_Scraped_1.csv.\n",
      "Processing batch: rows 172 to 172\n",
      "Setting up Chrome WebDriver...\n",
      "==================================================\n",
      "ACTION REQUIRED:\n",
      "Please log in to your LinkedIn account in the browser window.\n",
      "(If you are already logged in, just press Enter).\n",
      "Your session will be saved for future runs.\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Press Enter here to continue AFTER you are successfully logged in... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login confirmed. Starting scraper...\n",
      "\n",
      "--- Processing The Aspen Institute (nan) ---\n",
      "Skipping: Invalid or missing LinkedIn URL.\n",
      "\n",
      "Scraping complete for batch. 1 rows processed.\n",
      "\n",
      "An unexpected error occurred: name 'OUTPUT_File' is not defined\n",
      "Cleaning up WebDriver...\n"
     ]
    }
   ],
   "source": [
    "'''Extracts Jobs and their details'''\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "from urllib.parse import urljoin\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException, StaleElementReferenceException\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# --- Parameters You Can Change ---\n",
    "\n",
    "# Path to your input CSV file\n",
    "CSV_FILE = 'GFI_Data_With_Careers.csv'\n",
    "\n",
    "# Name of the column containing the LinkedIn URLs\n",
    "LINKEDIN_COL = 'Linkedin URL'\n",
    "\n",
    "# Name of the column with the company name (for logging)\n",
    "COMPANY_COL = 'Company Name'\n",
    "\n",
    "# Set your batch processing parameters\n",
    "START_INDEX = 172  # Row to start from (0 is the first row)\n",
    "BATCH_SIZE = 1   # Number of rows to process\n",
    "\n",
    "# Define a single output file for all batches\n",
    "OUTPUT_FILE = 'GFI_Data_With_Jobs_Scraped_1.csv'\n",
    "\n",
    "# Define a path to store your LinkedIn profile/session\n",
    "LINKEDIN_PROFILE_PATH = os.path.join(os.getcwd(), 'linkedin_profile')\n",
    "\n",
    "# ---------------------------------\n",
    "\n",
    "def setup_driver():\n",
    "    \"\"\"Sets up the Chrome WebDriver.\"\"\"\n",
    "    print(\"Setting up Chrome WebDriver...\")\n",
    "    # Set up Chrome options\n",
    "    options = webdriver.ChromeOptions()\n",
    "    \n",
    "    # Add user-data-dir to save your login session\n",
    "    options.add_argument(f\"--user-data-dir={LINKEDIN_PROFILE_PATH}\")\n",
    "    \n",
    "    options.add_argument(\"--start-maximized\")\n",
    "    options.add_argument(\"--disable-infobars\")\n",
    "    options.add_argument(\"--disable-extensions\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    # Using webdriver-manager to automatically handle the driver\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    driver = webdriver.Chrome(service=service, options=options)\n",
    "    return driver\n",
    "\n",
    "def scrape_linkedin_jobs(driver, batch_df):\n",
    "    \"\"\"\n",
    "    Scrapes job data from LinkedIn company pages.\n",
    "    \n",
    "    This function modifies a copy of the batch DataFrame and returns it.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Handle Login\n",
    "    driver.get(\"https://www.linkedin.com/login\")\n",
    "    print(\"=\"*50)\n",
    "    print(\"ACTION REQUIRED:\")\n",
    "    print(\"Please log in to your LinkedIn account in the browser window.\")\n",
    "    print(\"(If you are already logged in, just press Enter).\")\n",
    "    print(\"Your session will be saved for future runs.\")\n",
    "    print(\"=\"*50)\n",
    "    input(\"Press Enter here to continue AFTER you are successfully logged in...\")\n",
    "    print(\"Login confirmed. Starting scraper...\")\n",
    "    \n",
    "    # Create a copy of the batch to modify.\n",
    "    batch_copy = batch_df.copy()\n",
    "    \n",
    "    # Get the batch of rows to process\n",
    "    for index, row in batch_copy.iterrows():\n",
    "        company_name = row.get(COMPANY_COL, f\"Row {index}\")\n",
    "        linkedin_url = row.get(LINKEDIN_COL)\n",
    "        \n",
    "        print(f\"\\n--- Processing {company_name} ({linkedin_url}) ---\")\n",
    "        \n",
    "        # 2. Validate URL\n",
    "        if not isinstance(linkedin_url, str) or 'linkedin.com/company/' not in linkedin_url:\n",
    "            print(f\"Skipping: Invalid or missing LinkedIn URL.\")\n",
    "            continue\n",
    "            \n",
    "        # 3. Construct jobs URL and navigate\n",
    "        # We add ?lang=en to force English, making selectors more consistent\n",
    "        jobs_url = linkedin_url.rstrip('/') + '/jobs/?lang=en'\n",
    "        \n",
    "        try:\n",
    "            driver.get(jobs_url)\n",
    "            # Wait for the first job card to load\n",
    "            WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, \"li[class*='job-card-container']\"))\n",
    "            )\n",
    "            print(\"Jobs page loaded.\")\n",
    "            time.sleep(1) # Extra buffer for elements to render\n",
    "            \n",
    "        except TimeoutException:\n",
    "            print(\"Skipping: Could not find job list (page timed out or no 'jobs' section).\")\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping: Error loading jobs page: {e}\")\n",
    "            continue\n",
    "\n",
    "        # 4. Find job list items\n",
    "        try:\n",
    "            # Scroll down to load more jobs (optional, but can help)\n",
    "            driver.execute_script(\"window.scrollTo(0, 500);\")\n",
    "            time.sleep(1.5)\n",
    "\n",
    "            job_elements = driver.find_elements(By.CSS_SELECTOR, \"li[class*='job-card-container']\")\n",
    "            \n",
    "            if not job_elements:\n",
    "                print(\"No job postings found on the page.\")\n",
    "                continue\n",
    "                \n",
    "            print(f\"Found {len(job_elements)} job postings. Extracting top 3...\")\n",
    "            \n",
    "            # Loop by index (0, 1, 2) to prevent stale element errors\n",
    "            job_limit = min(len(job_elements), 3)\n",
    "            \n",
    "            for job_index in range(job_limit):\n",
    "                \n",
    "                job_num = job_index + 1\n",
    "                \n",
    "                # Re-find the list in *every* loop iteration\n",
    "                try:\n",
    "                    all_job_elements = driver.find_elements(By.CSS_SELECTOR, \"li[class*='job-card-container']\")\n",
    "                    if job_index >= len(all_job_elements):\n",
    "                        print(\"  - Job list changed, stopping early.\")\n",
    "                        break\n",
    "                    job_element = all_job_elements[job_index]\n",
    "                except Exception as e:\n",
    "                    print(f\"  - Error re-finding job list: {e}. Skipping remaining jobs.\")\n",
    "                    break\n",
    "                \n",
    "                try:\n",
    "                    # 6. CLICK the job item from the left list\n",
    "                    job_element.click()\n",
    "                    \n",
    "                    # Wait for the description pane (right side) to update.\n",
    "                    time.sleep(3.5) \n",
    "                    \n",
    "                    # 7. EXTRACT ALL DETAILS from the right pane\n",
    "                    job_title = \"Not Found\"\n",
    "                    job_url = \"Not Found\"\n",
    "                    job_location = \"Not Found\"\n",
    "                    job_date = \"Not Found\"\n",
    "                    job_description = \"Not Found\"\n",
    "                    \n",
    "                    # Get Title and URL from right pane H1 tag\n",
    "                    try:\n",
    "                        title_element = driver.find_element(By.CSS_SELECTOR, \"h1[class*='t-24']\")\n",
    "                        job_title = title_element.text.strip()\n",
    "                        try:\n",
    "                            link_element = title_element.find_element(By.TAG_NAME, \"a\")\n",
    "                            relative_url = link_element.get_attribute('href')\n",
    "                            job_url = urljoin(\"https://www.linkedin.com\", relative_url)\n",
    "                        except NoSuchElementException:\n",
    "                            print(\"  - Could not find job URL link inside title\")\n",
    "                    except NoSuchElementException:\n",
    "                        print(\"  - Could not find job title (h1.t-24)\")\n",
    "\n",
    "                    # Get Location and Date from details container\n",
    "                    try:\n",
    "                        details_container = driver.find_element(By.CSS_SELECTOR, \"div[class*='job-details-jobs-unified-top-card__tertiary-description']\")\n",
    "                        details_text = details_container.text\n",
    "                        parts = details_text.split('·')\n",
    "                        \n",
    "                        if len(parts) > 0:\n",
    "                            job_location = parts[0].strip()\n",
    "                        if len(parts) > 1:\n",
    "                            job_date = parts[1].strip()\n",
    "                            \n",
    "                    except NoSuchElementException:\n",
    "                        print(\"  - Could not find details container (location/date)\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"  - Error parsing location/date: {e}\")\n",
    "\n",
    "                    # Get description from its container (and handle \"Show More\")\n",
    "                    try:\n",
    "                        # 1. Find the new description container by its ID\n",
    "                        description_container = driver.find_element(By.ID, \"job-details\")\n",
    "                        \n",
    "                        # 2. Try to find and click the \"Show more\" button\n",
    "                        try:\n",
    "                            # The button is *within* the \"job-details\" container's parent\n",
    "                            article_container = driver.find_element(By.CLASS_NAME, \"jobs-description__container\")\n",
    "                            show_more_button = article_container.find_element(By.CSS_SELECTOR, \"button[class*='description__show-more-less-button'][aria-expanded='false']\")\n",
    "                            \n",
    "                            driver.execute_script(\"arguments[0].click();\", show_more_button)\n",
    "                            print(\"  - Clicked 'Show more' for description.\")\n",
    "                            time.sleep(1.0) # Wait for text to expand\n",
    "                        except NoSuchElementException:\n",
    "                            pass # Button not found, description is likely already expanded\n",
    "                        except Exception as e:\n",
    "                            print(f\"  - Note: Error clicking 'Show more' (might not exist): {e}\")\n",
    "                            \n",
    "                        # 3. Get the description text\n",
    "                        # Use .get_attribute('innerText') for robustness\n",
    "                        job_description = description_container.get_attribute('innerText').strip()\n",
    "                        \n",
    "                    except NoSuchElementException:\n",
    "                        print(f\"  - Could not find job description container (ID='job-details') for '{job_title}'\")\n",
    "                    \n",
    "                    # 8. Store results in the DataFrame copy\n",
    "                    batch_copy.at[index, f'job post{job_num} title'] = job_title\n",
    "                    batch_copy.at[index, f'job post{job_num} URL'] = job_url\n",
    "                    batch_copy.at[index, f'job post{job_num} location'] = job_location\n",
    "                    batch_copy.at[index, f'job post{job_num} date'] = job_date\n",
    "                    batch_copy.at[index, f'job post{job_num} description'] = job_description\n",
    "                    \n",
    "                    print(f\"  + Extracted job {job_num}: {job_title}\")\n",
    "                    \n",
    "                except StaleElementReferenceException:\n",
    "                    print(\"  - StaleElementReferenceException occurred. Skipping this job.\")\n",
    "                    continue\n",
    "                except Exception as e:\n",
    "                    print(f\"  - Error extracting details for one job: {e}\")\n",
    "                    # Try to recover by scrolling and waiting\n",
    "                    driver.execute_script(\"window.scrollBy(0, 100);\")\n",
    "                    time.sleep(1)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing job list for {company_name}: {e}\")\n",
    "\n",
    "    # Return the modified batch DataFrame\n",
    "    return batch_copy\n",
    "\n",
    "def main():\n",
    "    \n",
    "    # --- Load Data ---\n",
    "    # **NEW FIX: Check if the output file already exists to preserve old data**\n",
    "    if os.path.exists(OUTPUT_FILE):\n",
    "        print(f\"Found existing output file. Loading from: {OUTPUT_FILE}\")\n",
    "        input_file_to_load = OUTPUT_FILE\n",
    "    else:\n",
    "        print(f\"No existing output file found. Loading from original: {CSV_FILE}\")\n",
    "        input_file_to_load = CSV_FILE\n",
    "        \n",
    "    try:\n",
    "        # Load the dataframe from the correct file (either original or previous output)\n",
    "        df = pd.read_csv(input_file_to_load)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERROR: Input file not found: {input_file_to_load}\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Could not read CSV: {e}\")\n",
    "        return\n",
    "        \n",
    "    print(f\"Loaded {len(df)} rows from {input_file_to_load}.\")\n",
    "    \n",
    "    # Calculate end index\n",
    "    end_index = min(START_INDEX + BATCH_SIZE, len(df))\n",
    "    print(f\"Processing batch: rows {START_INDEX} to {end_index - 1}\")\n",
    "\n",
    "    if START_INDEX >= len(df):\n",
    "        print(\"Start index is beyond the file length. Nothing to process.\")\n",
    "        return\n",
    "        \n",
    "    # Select the batch of rows to process\n",
    "    batch_df = df.iloc[START_INDEX:end_index]\n",
    "    \n",
    "    # --- Run Scraper ---\n",
    "    driver = None\n",
    "    if batch_df.empty:\n",
    "        print(\"Batch is empty. Nothing to process.\")\n",
    "        return\n",
    "        \n",
    "    try:\n",
    "        driver = setup_driver()\n",
    "        # Scraper returns the processed batch\n",
    "        scraped_data_df = scrape_linkedin_jobs(driver, batch_df)\n",
    "        \n",
    "        if driver:\n",
    "            driver.quit()\n",
    "        \n",
    "        # --- Save Results ---\n",
    "        if scraped_data_df.empty:\n",
    "            print(\"No data was scraped in this batch.\")\n",
    "            return\n",
    "\n",
    "        # Ensure new columns from the scraped batch exist in the main DataFrame\n",
    "        new_cols = [col for col in scraped_data_df.columns if col not in df.columns]\n",
    "        \n",
    "        if new_cols:\n",
    "            print(f\"Adding new columns to main DataFrame: {new_cols}\")\n",
    "            for col in new_cols:\n",
    "                df[col] = pd.NA\n",
    "        \n",
    "        # Update the main DataFrame with the new data from the batch\n",
    "        df.update(scraped_data_df)\n",
    "        \n",
    "        # Save the *entire* updated DataFrame\n",
    "        df.to_csv(OUTPUT_FILE, index=False, encoding='utf-8')\n",
    "        print(f\"\\nScraping complete for batch. {len(scraped_data_df)} rows processed.\")\n",
    "        print(f\"All data saved to {OUTPUT_File}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn unexpected error occurred: {e}\")\n",
    "    finally:\n",
    "        if driver:\n",
    "            print(\"Cleaning up WebDriver...\")\n",
    "            driver.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # To run this in Jupyter, you would copy the contents of main()\n",
    "    # and the helper functions into cells and then call main() in the last cell.\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "23e0f347-da3a-4f3f-8f85-00dee622e92a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Job Extraction Report ---\n",
      "Jobs found for 'job post1 title': 118\n",
      "Jobs found for 'job post2 title': 107\n",
      "Jobs found for 'job post3 title': 104\n",
      "-----------------------------\n",
      "Total jobs extracted: 329\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "OUTPUT_FILE = 'GFI_Data_With_Jobs_Scraped_1.csv'\n",
    "\n",
    "if not os.path.exists(OUTPUT_FILE):\n",
    "    print(f\"Output file not found: {OUTPUT_FILE}\")\n",
    "    print(\"Please run the scraper script at least once.\")\n",
    "else:\n",
    "    try:\n",
    "        # Load the saved data\n",
    "        df = pd.read_csv(OUTPUT_FILE)\n",
    "        \n",
    "        # Define the job title columns\n",
    "        job_cols = ['job post1 title', 'job post2 title', 'job post3 title']\n",
    "        \n",
    "        total_jobs_extracted = 0\n",
    "        \n",
    "        print(\"--- Job Extraction Report ---\")\n",
    "        \n",
    "        for col in job_cols:\n",
    "            if col in df.columns:\n",
    "                # Count where the column is not null AND not 'Not Found'\n",
    "                count = df[\n",
    "                    (df[col].notna()) & (df[col] != 'Not Found')\n",
    "                ].shape[0]\n",
    "                \n",
    "                print(f\"Jobs found for '{col}': {count}\")\n",
    "                total_jobs_extracted += count\n",
    "            else:\n",
    "                print(f\"Column '{col}' not found in the file yet.\")\n",
    "                \n",
    "        print(\"-----------------------------\")\n",
    "        print(f\"Total jobs extracted: {total_jobs_extracted}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while reading the file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "8655b89d-7f1c-4d41-b9ea-b2fb48bab383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from your most recent output file: GFI_Data_With_Jobs_Scraped_1.csv\n",
      "Loaded 173 total company rows.\n",
      "--- URL Extraction Report ---\n",
      "Total 'Website URL': 170 (out of 173)\n",
      "Total 'Linkedin URL': 161 (out of 173)\n",
      "Total 'Careers Page URL': 109 (out of 173)\n",
      "Total 'Job listings page URL': 106 (out of 173)\n",
      "-----------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# The final output file, which contains all data\n",
    "OUTPUT_FILE = 'GFI_Data_With_Jobs_Scraped_1.csv'\n",
    "# The input file, as a fallback\n",
    "INPUT_FILE = 'GFI_Data_With_Careers.csv'\n",
    "\n",
    "# Determine which file to load\n",
    "file_to_load = \"\"\n",
    "if os.path.exists(OUTPUT_FILE):\n",
    "    file_to_load = OUTPUT_FILE\n",
    "    print(f\"Loading data from your most recent output file: {OUTPUT_FILE}\")\n",
    "elif os.path.exists(INPUT_FILE):\n",
    "    file_to_load = INPUT_FILE\n",
    "    print(f\"Loading data from your original input file: {INPUT_FILE}\")\n",
    "else:\n",
    "    file_to_load = None\n",
    "    print(f\"ERROR: Neither {OUTPUT_FILE} nor {INPUT_FILE} could be found.\")\n",
    "\n",
    "if file_to_load:\n",
    "    try:\n",
    "        df = pd.read_csv(file_to_load)\n",
    "        \n",
    "        # Get total rows for context\n",
    "        total_rows = len(df)\n",
    "        print(f\"Loaded {total_rows} total company rows.\")\n",
    "        print(\"--- URL Extraction Report ---\")\n",
    "        \n",
    "        # List of columns to check\n",
    "        # (I noticed you typed \"linkedin urls\" twice, \n",
    "        # so I'm assuming you meant all the main URL columns)\n",
    "        url_columns = [\n",
    "            'Website URL',\n",
    "            'Linkedin URL',\n",
    "            'Careers Page URL',\n",
    "            'Job listings page URL'\n",
    "        ]\n",
    "        \n",
    "        for col in url_columns:\n",
    "            if col in df.columns:\n",
    "                # Count non-null (notna) values. This is the most reliable way.\n",
    "                count = df[col].notna().sum()\n",
    "                print(f\"Total '{col}': {count} (out of {total_rows})\")\n",
    "            else:\n",
    "                print(f\"Column '{col}' not found in the file.\")\n",
    "                \n",
    "        print(\"-----------------------------\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while reading the file: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d33bc2-3c6c-4265-96c3-b0a7bb9db7bc",
   "metadata": {},
   "source": [
    "# Growth For Impact - Web Scraping Assignment\n",
    "\n",
    "## Overview\n",
    "This project implements a multi-stage web scraping pipeline to extract comprehensive job posting information from climate-focused companies. Starting with only company names and descriptions, the system progressively gathers online presence data, career pages, and detailed job listings.\n",
    "\n",
    "---\n",
    "\n",
    "## Methodology\n",
    "\n",
    "### Stage 1: Website & LinkedIn URL Extraction\n",
    "\n",
    "**Objective:** Find official company websites and LinkedIn profiles using company name and description.\n",
    "\n",
    "**Implementation:** `WebsiteFinderSelenium` class\n",
    "\n",
    "**Process:**\n",
    "1. **Search Query Construction**\n",
    "   - Extracts 2-3 key descriptive words from company description\n",
    "   - Filters out common stop words (the, a, and, etc.)\n",
    "   - Creates targeted Google search queries combining company name with keywords\n",
    "   - Example: `\"Polestar driving society uncompromised official website\"`\n",
    "\n",
    "2. **Intelligent Web Search**\n",
    "   - Uses Selenium WebDriver to perform real Google searches\n",
    "   - Implements anti-detection measures:\n",
    "     - Custom user agents\n",
    "     - Disabled automation flags\n",
    "     - Realistic browser behavior with delays\n",
    "   - Separate searches for website and LinkedIn URLs\n",
    "\n",
    "3. **Result Extraction**\n",
    "   - Parses multiple HTML elements (cite tags, result links, search links)\n",
    "   - Validates URLs by checking accessibility\n",
    "   - Attempts alternative formats (www prefix) if initial validation fails\n",
    "\n",
    "4. **Progressive Processing**\n",
    "   - Batch processing (8 companies at a time) to avoid CAPTCHA\n",
    "   - Saves progress every 3 companies\n",
    "   - 8-second delays between searches to mimic human behavior\n",
    "   - Resumes from last checkpoint if interrupted\n",
    "\n",
    "**Key Features:**\n",
    "- Context-aware searching using company descriptions\n",
    "- Robust URL validation and normalization\n",
    "- Session persistence for interrupted runs\n",
    "- Rate limiting to avoid detection\n",
    "\n",
    "---\n",
    "\n",
    "### Stage 2: Career Page & Job Listings URL Extraction\n",
    "\n",
    "**Objective:** Locate career pages and job listing portals from company websites.\n",
    "\n",
    "**Implementation:** `CareerPageFinder` class\n",
    "\n",
    "**Process:**\n",
    "1. **Website Crawling**\n",
    "   - Navigates to company website using Selenium\n",
    "   - Extracts all links from the page\n",
    "   - Handles cookie consent popups automatically\n",
    "\n",
    "2. **Multi-Tier Detection System**\n",
    "\n",
    "   **Priority 1 - Known Job Platforms (Score: 10)**\n",
    "   - Detects 20+ ATS platforms: Lever, Greenhouse, Workable, Ashby, BambooHR, etc.\n",
    "   - Automatically identifies embedded job boards\n",
    "\n",
    "   **Priority 2 - Strong Career Keywords in URL (Score: 6-8)**\n",
    "   - URL patterns: `/careers`, `/jobs`, `/join-us`, `/hiring`, `/work-with-us`\n",
    "   - Cross-validates with link text content\n",
    "   \n",
    "   **Priority 3 - Link Text Analysis (Score: 5)**\n",
    "   - Searches for phrases: \"career\", \"jobs\", \"join us\", \"work with us\", \"opportunities\"\n",
    "\n",
    "3. **Job Listings Page Discovery**\n",
    "   - Follows redirect links with phrases like \"view jobs\", \"open positions\"\n",
    "   - Detects embedded iframes containing job boards\n",
    "   - Checks for job listing indicators in page content:\n",
    "     - \"apply now\", \"job title\", \"job description\", \"open position\"\n",
    "   - Attempts `/jobs` suffix on job platform URLs\n",
    "   - Falls back to career page if job-specific page not found\n",
    "\n",
    "4. **URL Filtering**\n",
    "   - Excludes individual job postings (patterns like `/job/1234`, `/apply/123`)\n",
    "   - Filters out non-career pages (privacy, terms, blog, news)\n",
    "   - Validates all URLs before saving\n",
    "\n",
    "**Key Features:**\n",
    "- Hierarchical scoring system for link relevance\n",
    "- Intelligent redirect following\n",
    "- Iframe detection for embedded job boards\n",
    "- Content-based heuristics for validation\n",
    "- Common URL pattern testing\n",
    "\n",
    "---\n",
    "\n",
    "### Stage 3: Job Details Extraction from LinkedIn\n",
    "\n",
    "**Objective:** Scrape detailed job information from LinkedIn company pages.\n",
    "\n",
    "**Implementation:** Main scraping function with Selenium\n",
    "\n",
    "**Process:**\n",
    "1. **Authentication & Session Management**\n",
    "   - One-time manual LinkedIn login\n",
    "   - Session cookies saved in `linkedin_profile` directory\n",
    "   - Automatic session reuse for subsequent runs\n",
    "   - User-data-dir profile persistence\n",
    "\n",
    "2. **Jobs Page Navigation**\n",
    "   - Constructs jobs URL: `{linkedin_company_url}/jobs/?lang=en`\n",
    "   - Forces English language for consistent selectors\n",
    "   - Waits for job card elements to load (10-second timeout)\n",
    "   - Performs scroll actions to trigger lazy-loaded content\n",
    "\n",
    "3. **Job Card Iteration**\n",
    "   - Identifies job cards using CSS selector: `li[class*='job-card-container']`\n",
    "   - Processes top 3 jobs per company\n",
    "   - Re-finds job list in each iteration to prevent stale element errors\n",
    "   - Clicks each job card to load details in right pane\n",
    "\n",
    "4. **Detailed Information Extraction**\n",
    "   \n",
    "   For each job, extracts:\n",
    "   \n",
    "   **Job Title & URL**\n",
    "   - Selector: `h1[class*='t-24']`\n",
    "   - Extracts embedded link using `urljoin` for absolute URLs\n",
    "   \n",
    "   **Location & Date Posted**\n",
    "   - Selector: `div[class*='job-details-jobs-unified-top-card__tertiary-description']`\n",
    "   - Parses text split by '·' separator\n",
    "   \n",
    "   **Job Description**\n",
    "   - Selector: `#job-details`\n",
    "   - Clicks \"Show more\" button if present: `button[class*='description__show-more-less-button'][aria-expanded='false']`\n",
    "   - Extracts full expanded text using `innerText` attribute\n",
    "\n",
    "5. **Data Storage Structure**\n",
    "   ```\n",
    "   job post1 title, job post1 URL, job post1 location, job post1 date, job post1 description\n",
    "   job post2 title, job post2 URL, job post2 location, job post2 date, job post2 description\n",
    "   job post3 title, job post3 URL, job post3 location, job post3 date, job post3 description\n",
    "   ```\n",
    "\n",
    "6. **Error Handling**\n",
    "   - `StaleElementReferenceException`: Re-finds elements and continues\n",
    "   - `TimeoutException`: Skips company if jobs page doesn't load\n",
    "   - Missing elements: Records \"Not Found\" for specific fields\n",
    "   - Recovers from errors with scroll actions and wait periods\n",
    "\n",
    "**Key Features:**\n",
    "- Session persistence eliminates repeated logins\n",
    "- Robust element re-finding strategy\n",
    "- Dynamic content expansion (Show more buttons)\n",
    "- Comprehensive error recovery\n",
    "- Batch processing with progress preservation\n",
    "\n",
    "---\n",
    "\n",
    "## Technical Architecture\n",
    "\n",
    "### Technologies Used\n",
    "- **Python 3.12**\n",
    "- **Selenium WebDriver** - Browser automation\n",
    "- **Pandas** - Data manipulation and CSV handling\n",
    "- **BeautifulSoup4** - HTML parsing (auxiliary)\n",
    "- **webdriver-manager** - Automatic ChromeDriver management\n",
    "- **Requests** - URL validation\n",
    "\n",
    "### Anti-Detection Measures\n",
    "- Randomized user agents\n",
    "- Disabled automation flags\n",
    "- CDP commands to hide webdriver property\n",
    "- Human-like delays (3-8 seconds between actions)\n",
    "- Batch processing to avoid rate limits\n",
    "- Session persistence to reduce suspicious activity\n",
    "\n",
    "### Data Persistence Strategy\n",
    "- **Single Output File Approach**: All batches update the same CSV\n",
    "- **Incremental Updates**: Only processes missing data\n",
    "- **Checkpoint System**: Saves every 3 companies\n",
    "- **Resume Capability**: Continues from `START_INDEX` parameter\n",
    "- **Data Validation**: Checks for existing data before scraping\n",
    "\n",
    "---\n",
    "\n",
    "# Run main() - will prompt for LinkedIn login on first run\n",
    "```\n",
    "\n",
    "### Data Quality Features\n",
    "- Validated URLs (accessibility checked)\n",
    "- Cleaned and normalized data\n",
    "- Comprehensive error handling\n",
    "- \"Not Found\" placeholders for missing data\n",
    "- UTF-8 encoding support\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5dd9c06-6ad6-4413-b7ad-e5ef6223370d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
